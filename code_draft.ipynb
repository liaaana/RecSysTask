{"cells":[{"cell_type":"markdown","metadata":{},"source":["# LightGCN and NGCF\n","I tried applying LightGCN and NGCF to my recommendation system. Although the results were not quite great."]},{"cell_type":"markdown","metadata":{},"source":["# Import necessary libraries"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch_geometric in /opt/anaconda3/lib/python3.12/site-packages (2.7.0)\n","Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (3.9.5)\n","Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (2024.3.1)\n","Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (3.1.4)\n","Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (1.26.4)\n","Requirement already satisfied: psutil>=5.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (5.9.0)\n","Requirement already satisfied: pyparsing in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (3.0.9)\n","Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (2.32.2)\n","Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (4.66.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.2.0)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (23.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.4.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.9.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch_geometric) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (2.2.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (2024.6.2)\n"]}],"source":["!pip install torch_geometric"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T01:18:37.859370Z","iopub.status.busy":"2024-09-30T01:18:37.858894Z","iopub.status.idle":"2024-09-30T01:18:37.869251Z","shell.execute_reply":"2024-09-30T01:18:37.866940Z","shell.execute_reply.started":"2024-09-30T01:18:37.859323Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from abc import ABC, abstractmethod\n","from typing import Dict, List\n","import pickle\n","from scipy.sparse import csr_matrix\n","from sklearn.preprocessing import LabelEncoder\n","from tqdm.auto import tqdm\n","import networkx as nx\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch_geometric\n","from torch_geometric.nn.conv import MessagePassing\n","from torch_geometric.utils import degree\n","from sklearn import preprocessing as pp\n","import scipy.sparse as sp\n","\n","np.random.seed(42)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Given code"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T01:18:37.890182Z","iopub.status.busy":"2024-09-30T01:18:37.889679Z","iopub.status.idle":"2024-09-30T01:18:37.904874Z","shell.execute_reply":"2024-09-30T01:18:37.903442Z","shell.execute_reply.started":"2024-09-30T01:18:37.890126Z"},"trusted":true},"outputs":[],"source":["# ACHTUNG! DO NOT TOUCH\n","\n","def ndcg_metric(gt_items: np.ndarray, predicted: np.ndarray) -> float:\n","    at = len(predicted)\n","    relevance = np.array([1 if x in predicted else 0 for x in gt_items])\n","    # DCG uses the relevance of the recommended items\n","    rank_dcg = dcg(relevance)\n","    if rank_dcg == 0.0:\n","        return 0.0\n","\n","    # IDCG has all relevances to 1 (or the values provided), up to the number of items in the test set that can fit in the list length\n","    ideal_dcg = dcg(np.sort(relevance)[::-1][:at])\n","\n","    if ideal_dcg == 0.0:\n","        return 0.0\n","\n","    ndcg_ = rank_dcg / ideal_dcg\n","\n","    return ndcg_\n","\n","\n","def dcg(scores: np.ndarray) -> float:\n","    return np.sum(\n","        np.divide(np.power(2, scores) - 1, np.log2(np.arange(scores.shape[0], dtype=np.float64) + 2)), dtype=np.float64\n","    )\n","\n","\n","def recall_metric(gt_items: np.ndarray, predicted: np.ndarray) -> float:\n","    n_gt = len(gt_items)\n","    intersection = len(set(gt_items).intersection(set(predicted)))\n","    return intersection / n_gt\n","\n","\n","def evaluate_recommender(df: pd.DataFrame, model_preds_col: str, gt_col: str = \"movie_id\") -> Dict[str, float]:\n","    metric_values = []\n","\n","    for _, row in df.iterrows():\n","        metric_values.append(\n","            (ndcg_metric(row[gt_col], row[model_preds_col]), recall_metric(row[gt_col], row[model_preds_col]))\n","        )\n","\n","    return {\"ndcg\": np.mean([x[0] for x in metric_values]), \"recall\": np.mean([x[1] for x in metric_values])}"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T01:18:37.927977Z","iopub.status.busy":"2024-09-30T01:18:37.927519Z","iopub.status.idle":"2024-09-30T01:18:37.937880Z","shell.execute_reply":"2024-09-30T01:18:37.936196Z","shell.execute_reply.started":"2024-09-30T01:18:37.927932Z"},"trusted":true},"outputs":[],"source":["class BaseRecommender(ABC):\n","    def __init__(self):\n","        self.trained = False\n","\n","    @abstractmethod\n","    def fit(self, df: pd.DataFrame) -> None:\n","        # реализация может быть любой, никаких ограничений\n","        # не забудьте про\n","        self.trained = True\n","\n","    @abstractmethod\n","    def predict(self, df: pd.DataFrame, topn: int = 10) -> List[np.ndarray]:\n","        # реализация может быть любой, НО\n","        # должен возвращать список массивов из movie_id, которые есть в `item_df`, чтобы корректно работал подсчет метрик\n","        pass"]},{"cell_type":"markdown","metadata":{},"source":["# Data preprocessing"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T01:18:53.181652Z","iopub.status.busy":"2024-09-30T01:18:53.181220Z","iopub.status.idle":"2024-09-30T01:18:56.252118Z","shell.execute_reply":"2024-09-30T01:18:56.250889Z","shell.execute_reply.started":"2024-09-30T01:18:53.181607Z"},"id":"9fKAfWyCm5eY","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["998 4483\n","400153\n","31821\n","Number of Unique Users :  998\n","Number of unique Items :  4483\n"]}],"source":["file_path = 'train_data.csv'\n","train_df = pd.read_csv(file_path)\n","\n","file_path = 'test_data.csv' \n","test_df = pd.read_csv(file_path)\n","\n","train_df = train_df[(train_df['user_id'] < 1000)].copy()\n","le_user = pp.LabelEncoder()\n","le_item = pp.LabelEncoder()\n","train_df['user_id_idx'] = le_user.fit_transform(train_df['user_id'].values)\n","train_df['movie_id_idx'] = le_item.fit_transform(train_df['movie_id'].values)\n","\n","\n","train_user_ids = train_df['user_id'].unique()\n","train_item_ids = train_df['movie_id'].unique()\n","\n","print(len(train_user_ids), len(train_item_ids))\n","\n","print(len(test_df))\n","test_df = test_df[(test_df['user_id'].isin(train_user_ids)) & (test_df['movie_id'].isin(train_item_ids))]\n","print(len(test_df))\n","\n","test_df['user_id_idx'] = le_user.transform(test_df['user_id'].values)\n","test_df['movie_id_idx'] = le_item.transform(test_df['movie_id'].values)\n","\n","\n","n_users = train_df['user_id_idx'].nunique()\n","n_items = train_df['movie_id_idx'].nunique()\n","print(\"Number of Unique Users : \", n_users)\n","print(\"Number of unique Items : \", n_items)"]},{"cell_type":"markdown","metadata":{},"source":["# Helper functions"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T01:18:56.255795Z","iopub.status.busy":"2024-09-30T01:18:56.255385Z","iopub.status.idle":"2024-09-30T01:18:56.270372Z","shell.execute_reply":"2024-09-30T01:18:56.268499Z","shell.execute_reply.started":"2024-09-30T01:18:56.255753Z"},"id":"NQRGy-CJnOkg","trusted":true},"outputs":[],"source":["def data_loader(data, batch_size, n_usr, n_itm):\n","    def sample_neg(x):\n","        while True:\n","            neg_id = np.random.randint(0, n_itm)\n","            if neg_id not in x:\n","                return neg_id\n","\n","\n","    interected_items_df = data.groupby('user_id_idx')['movie_id_idx'].apply(list).reset_index()\n","    indices = [x for x in range(n_usr)]\n","\n","    if n_usr < batch_size:\n","        users = [np.random.choice(indices) for _ in range(batch_size)]\n","    else:\n","        users = np.random.choice(indices, batch_size, replace=False)\n","\n","    users.sort()\n","    users_df = pd.DataFrame(users,columns = ['users'])\n","\n","    interected_items_df = pd.merge(interected_items_df, users_df, how = 'right', left_on = 'user_id_idx', right_on = 'users')\n","    pos_items = interected_items_df['movie_id_idx'].apply(lambda x : np.random.choice(x)).values\n","    neg_items = interected_items_df['movie_id_idx'].apply(lambda x: sample_neg(x)).values\n","\n","    return (\n","        torch.LongTensor(list(users)),\n","        torch.LongTensor(list(pos_items)) + n_usr,\n","        torch.LongTensor(list(neg_items)) + n_usr\n","    )\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T01:18:56.273208Z","iopub.status.busy":"2024-09-30T01:18:56.272729Z","iopub.status.idle":"2024-09-30T01:18:56.315127Z","shell.execute_reply":"2024-09-30T01:18:56.313841Z","shell.execute_reply.started":"2024-09-30T01:18:56.273166Z"},"id":"O3BkGyV9pkce","trusted":true},"outputs":[{"data":{"text/plain":["tensor([[ 642,  512,  158,  ..., 2181, 3494, 1836],\n","        [2215, 3753, 3533,  ...,  310,  707,  153]])"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["u_t = torch.LongTensor(train_df.user_id_idx.values)\n","i_t = torch.LongTensor(train_df.movie_id_idx.values) + n_users\n","train_edge_index = torch.stack((torch.cat([u_t, i_t]), torch.cat([i_t, u_t])))\n","train_edge_index"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T01:18:56.317082Z","iopub.status.busy":"2024-09-30T01:18:56.316699Z","iopub.status.idle":"2024-09-30T01:18:56.325443Z","shell.execute_reply":"2024-09-30T01:18:56.324125Z","shell.execute_reply.started":"2024-09-30T01:18:56.317042Z"},"trusted":true},"outputs":[],"source":["def compute_bpr_loss(users, users_emb, pos_emb, neg_emb, user_emb0, pos_emb0, neg_emb0):\n","  reg_loss = (1 / 2) * (\n","    user_emb0.norm().pow(2) +\n","    pos_emb0.norm().pow(2)  +\n","    neg_emb0.norm().pow(2)\n","  ) / float(len(users))\n","\n","  pos_scores = torch.mul(users_emb, pos_emb).sum(dim=1)\n","  neg_scores = torch.mul(users_emb, neg_emb).sum(dim=1)\n","\n","  bpr_loss = torch.mean(F.softplus(neg_scores - pos_scores))\n","\n","  return bpr_loss, reg_loss"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T01:18:56.385998Z","iopub.status.busy":"2024-09-30T01:18:56.385402Z","iopub.status.idle":"2024-09-30T01:18:56.401159Z","shell.execute_reply":"2024-09-30T01:18:56.399729Z","shell.execute_reply.started":"2024-09-30T01:18:56.385941Z"},"trusted":true},"outputs":[],"source":["def save_model(model, optimizer, epoch, loss, filepath='model.pkl'):\n","    torch.save({\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': loss,\n","    }, filepath)\n","    \n","def load_model(model, optimizer, filepath='model.pkl'):\n","    checkpoint = torch.load(filepath)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    loss = checkpoint['loss']\n","    return epoch, loss\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["latent_dim = 64\n","n_layers = 10\n","\n","EPOCHS = 30\n","BATCH_SIZE = 1024\n","DECAY = 0.0001\n","LR = 0.005\n","K = 10"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def train(model, optimizer, train_df, filepath):\n","  loss_list_epoch = []\n","  bpr_loss_list_epoch = []\n","  reg_loss_list_epoch = []\n","\n","  best_loss = float('inf')\n","  for epoch in tqdm(range(EPOCHS)):\n","      n_batch = int(len(train_df)/BATCH_SIZE)\n","\n","      final_loss_list = []\n","      bpr_loss_list = []\n","      reg_loss_list = []\n","\n","      model.train()\n","      for batch_idx in range(n_batch):\n","\n","          optimizer.zero_grad()\n","\n","          users, pos_items, neg_items = data_loader(train_df, BATCH_SIZE, n_users, n_items)\n","          users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0 = model.encode_minibatch(users, pos_items, neg_items, train_edge_index)\n","\n","          bpr_loss, reg_loss = compute_bpr_loss(\n","            users, users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0\n","          )\n","          reg_loss = DECAY * reg_loss\n","          final_loss = bpr_loss + reg_loss\n","\n","          final_loss.backward()\n","          optimizer.step()\n","\n","          final_loss_list.append(final_loss.item())\n","          bpr_loss_list.append(bpr_loss.item())\n","          reg_loss_list.append(reg_loss.item())\n","\n","      loss_list_epoch.append(round(np.mean(final_loss_list),4))\n","      bpr_loss_list_epoch.append(round(np.mean(bpr_loss_list),4))\n","      reg_loss_list_epoch.append(round(np.mean(reg_loss_list),4))\n","    \n","      if bpr_loss.item() < best_loss:\n","            best_loss = bpr_loss.item()\n","            save_model(model, optimizer, epoch, best_loss,  filepath)\n","\n","  return (\n","    loss_list_epoch,\n","    bpr_loss_list_epoch,\n","    reg_loss_list_epoch,\n","  )\n","\n","\n","def eval(model, n_users, n_items, train_df, test_df, K):\n","    model.eval()\n","    with torch.no_grad():\n","      _, out = model(train_edge_index)\n","      final_user_Embed, final_item_Embed = torch.split(out, (n_users, n_items))\n","        \n","    test_user_ids = torch.LongTensor(test_df['user_id_idx'].unique())\n","    relevance_score = torch.matmul(final_user_Embed, torch.transpose(final_item_Embed,0, 1))\n","    i = torch.stack((\n","    torch.LongTensor(train_df['user_id_idx'].values),\n","    torch.LongTensor(train_df['movie_id_idx'].values)\n","    ))\n","    v = torch.ones((len(train_df)), dtype=torch.float64)\n","    interactions_t = torch.sparse.FloatTensor(i, v, (n_users, n_items))\\\n","      .to_dense()\n","\n","    relevance_score = torch.mul(relevance_score, (1 - interactions_t))\n","\n","    topk_relevance_indices = torch.topk(relevance_score, K).indices\n","    topk_relevance_indices_df = pd.DataFrame(topk_relevance_indices.cpu().numpy(),columns =['top_indx_'+str(x+1) for x in range(K)])\n","    topk_relevance_indices_df['user_ID'] = topk_relevance_indices_df.index\n","    topk_relevance_indices_df['top_rlvnt_itm'] = topk_relevance_indices_df[['top_indx_'+str(x+1) for x in range(K)]].values.tolist()\n","    topk_relevance_indices_df = topk_relevance_indices_df[['user_ID','top_rlvnt_itm']]\n","    \n","    unique_user_ids = test_df['user_id_idx'].unique()\n","    \n","    all_recs = []\n","    \n","    for user_id in tqdm(unique_user_ids, desc=\"Predicting\", unit=\"user\"):\n","        recs_for_user = topk_relevance_indices_df.loc[topk_relevance_indices_df['user_ID'] == user_id, 'top_rlvnt_itm'].values[0]\n","        all_recs.append(recs_for_user)\n","    \n","    return all_recs\n"]},{"cell_type":"markdown","metadata":{},"source":["# LightGCN"]},{"cell_type":"markdown","metadata":{},"source":["### LightGCN Convolutional Layer\n","\n","The LightGCN architecture is governed by the following rules:\n","\n","$$e_{u}^{(k+1)} = \\sum\\limits_{i \\in N_u} \\frac{1}{\\sqrt{|N_u|}\\sqrt{|N_i|}}e^{(k)}_i$$\n","\n","$$e_{i}^{(k+1)} = \\sum\\limits_{u \\in N_i} \\frac{1}{\\sqrt{|N_i|}\\sqrt{|N_u|}}e^{(k)}_u$$\n","In essence, the embedding for each node after a single LightGCN layer is the sum of the synthetic normalized embeddings of it's neighbors before the layer."]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["class LightGCNRecommender(BaseRecommender, nn.Module):\n","    class LightGCNConv(MessagePassing):\n","        def __init__(self, **kwargs):\n","            super().__init__(aggr='add')\n","\n","        def forward(self, x, edge_index):\n","            from_, to_ = edge_index\n","            deg = degree(to_, x.size(0), dtype=x.dtype)\n","            deg_inv_sqrt = deg.pow(-0.5)\n","            deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n","            norm = deg_inv_sqrt[from_] * deg_inv_sqrt[to_]\n","\n","            return self.propagate(edge_index, x=x, norm=norm)\n","\n","        def message(self, x_j, norm):\n","            return norm.view(-1, 1) * x_j\n","\n","    def __init__(self, latent_dim, num_layers, num_users, num_items):\n","        super().__init__() \n","        nn.Module.__init__(self)\n","        self.embedding = nn.Embedding(num_users + num_items, latent_dim)\n","        self.convs = nn.ModuleList(self.LightGCNConv() for _ in range(num_layers))\n","        self.init_parameters()\n","\n","    def init_parameters(self):\n","        nn.init.normal_(self.embedding.weight, std=0.1)\n","\n","    def forward(self, edge_index):\n","        emb0 = self.embedding.weight\n","        embs = [emb0]\n","\n","        emb = emb0\n","        for conv in self.convs:\n","            emb = conv(x=emb, edge_index=edge_index)\n","            embs.append(emb)\n","\n","        out = torch.mean(torch.stack(embs, dim=0), dim=0)\n","        return emb0, out\n","\n","    def encode_minibatch(self, users, pos_items, neg_items, edge_index):\n","        emb0, out = self(edge_index)\n","        return (\n","            out[users],\n","            out[pos_items],\n","            out[neg_items],\n","            emb0[users],\n","            emb0[pos_items],\n","            emb0[neg_items]\n","        )\n","\n","    def fit(self, optimizer, train_df: pd.DataFrame, filepath) -> None:\n","        train(self, optimizer, train_df, filepath)\n","        self.trained = True \n","\n","    def predict(self, n_users, n_items, train_df: pd.DataFrame, test_df: pd.DataFrame,topn: int = 10) -> List[np.ndarray]:\n","        assert self.trained\n","        return eval(self, n_users, n_items, train_df, test_df, topn)\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-09-30T01:18:56.452738Z","iopub.status.busy":"2024-09-30T01:18:56.452221Z"},"id":"eKBv9eXongux","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2691c5480dbb43fda350e7e5d0268de4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/30 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/var/folders/2_/xgmb5cbs7sl790jf18j7rq640000gn/T/ipykernel_2136/3067261680.py:65: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:643.)\n","  interactions_t = torch.sparse.FloatTensor(i, v, (n_users, n_items))\\\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"372114e9bcc44ab4be31519b02baa9e4","version_major":2,"version_minor":0},"text/plain":["Predicting:   0%|          | 0/978 [00:00<?, ?user/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'ndcg': 0.023176415174354374, 'recall': 0.0023607010683388426}\n"]}],"source":["light_gcn_recommender = LightGCNRecommender(latent_dim=latent_dim, num_layers=n_layers, num_users=n_users, num_items=n_items)\n","optimizer = torch.optim.Adam(light_gcn_recommender.parameters(), lr=LR)\n","light_gcn_recommender.fit(optimizer, train_df, 'model_light_gcn.pkl')\n","test_df_agg = test_df.groupby(\"user_id\").agg({\"movie_id\": list}).reset_index()\n","test_df_agg[\"light_gcn_recs\"] = light_gcn_recommender.predict(n_users, n_items, train_df, test_df, K)\n","print(evaluate_recommender(test_df_agg, model_preds_col=\"light_gcn_recs\"))"]},{"cell_type":"markdown","metadata":{},"source":["# NGCF"]},{"cell_type":"markdown","metadata":{},"source":["### NGCF Layer\n","\n","NGCF is an older architecture than LightGCN. LightGCN functions the same as NGCF, but removes the learnable linear layers, non-linear activation, and dropout.\n","\n","One layer of NGCF updates user and item embeddings as follows:\n","\n","$$e_{u}^{(k+1)} = \\sigma\\left(W_1 e_u^{(k)} + \\sum\\limits_{i \\in N_u} \\frac{1}{\\sqrt{|N_u|}\\sqrt{|N_i|}}(W_1e^{(k)}_i + W_2(e^{(k)}_i \\odot e^{(k)}_u))\\right)$$\n","\n","$$e_{i}^{(k+1)} = \\sigma\\left(W_1 e_i^{(k)} + \\sum\\limits_{u \\in N_i} \\frac{1}{\\sqrt{|N_i|}\\sqrt{|N_u|}}(W_1e^{(k)}_u + W_2(e^{(k)}_u \\odot e^{(k)}_i))\\right)$$\n","\n","Typically, NGCF is implemented with dropout before the activation and with an activation function $\\sigma$ of LeakyReLU."]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["class NGCFRecommender(BaseRecommender, nn.Module):\n","    class NGCFConv(MessagePassing):\n","        def __init__(self, latent_dim, dropout, bias=True, **kwargs):\n","            super().__init__(aggr='add', **kwargs)\n","            self.dropout = dropout\n","\n","            self.lin_1 = nn.Linear(latent_dim, latent_dim, bias=bias)\n","            self.lin_2 = nn.Linear(latent_dim, latent_dim, bias=bias)\n","\n","            self.init_parameters()\n","\n","        def init_parameters(self):\n","            nn.init.xavier_uniform_(self.lin_1.weight)\n","            nn.init.xavier_uniform_(self.lin_2.weight)\n","\n","        def forward(self, x, edge_index):\n","            from_, to_ = edge_index\n","            deg = degree(to_, x.size(0), dtype=x.dtype)\n","            deg_inv_sqrt = deg.pow(-0.5)\n","            deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n","            norm = deg_inv_sqrt[from_] * deg_inv_sqrt[to_]\n","            \n","            out = self.propagate(edge_index, x=(x, x), norm=norm)\n","            out += self.lin_1(x)\n","            out = F.dropout(out, self.dropout, self.training)\n","            return F.leaky_relu(out)\n","\n","        def message(self, x_j, x_i, norm):\n","            return norm.view(-1, 1) * (self.lin_1(x_j) + self.lin_2(x_j * x_i))\n","\n","    def __init__(self, latent_dim, num_layers, num_users, num_items, dropout=0.1):\n","        super().__init__() \n","        nn.Module.__init__(self)\n","        self.embedding = nn.Embedding(num_users + num_items, latent_dim)\n","        self.convs = nn.ModuleList(self.NGCFConv(latent_dim, dropout=dropout) for _ in range(num_layers))\n","        self.init_parameters()\n","\n","    def init_parameters(self):\n","        nn.init.xavier_uniform_(self.embedding.weight, gain=1)\n","\n","    def forward(self, edge_index):\n","        emb0 = self.embedding.weight\n","        embs = [emb0]\n","        emb = emb0\n","    \n","        for conv in self.convs:\n","            emb = conv(x=emb, edge_index=edge_index)\n","            embs.append(emb)\n","\n","        out = torch.cat(embs, dim=-1) \n","        return emb0, out\n","\n","    def encode_minibatch(self, users, pos_items, neg_items, edge_index):\n","\n","        emb0, out = self(edge_index)\n","        return (\n","            out[users],\n","            out[pos_items],\n","            out[neg_items],\n","            emb0[users],\n","            emb0[pos_items],\n","            emb0[neg_items]\n","        )\n","\n","    def fit(self, optimizer, train_df: pd.DataFrame, filepath) -> None:\n","        train(self, optimizer, train_df, filepath)\n","        self.trained = True \n","\n","    def predict(self, n_users, n_items, train_df: pd.DataFrame, test_df: pd.DataFrame, topn: int = 10) -> List[np.ndarray]:\n","        assert self.trained\n","        return eval(self, n_users, n_items, train_df, test_df, topn)"]},{"cell_type":"code","execution_count":25,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e86a81d573a84ae8ad5156f864a08620","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/30 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e38b6e2314524ebcae625da74826777e","version_major":2,"version_minor":0},"text/plain":["Predicting:   0%|          | 0/978 [00:00<?, ?user/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'ndcg': 0.004980542170029264, 'recall': 0.00044213522094688375}\n"]}],"source":["ngcf_recommender = NGCFRecommender(latent_dim=latent_dim, num_layers=n_layers, num_users=n_users, num_items=n_items)\n","optimizer = torch.optim.Adam(ngcf_recommender.parameters(), lr=LR)\n","ngcf_recommender.fit(optimizer, train_df,'model_ngcf.pkl')\n","test_df_agg = test_df.groupby(\"user_id\").agg({\"movie_id\": list}).reset_index()\n","test_df_agg[\"ngcf_recs\"] = ngcf_recommender.predict(n_users, n_items, train_df, test_df, K)\n","print(evaluate_recommender(test_df_agg, model_preds_col=\"ngcf_recs\"))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5783706,"sourceId":9503117,"sourceType":"datasetVersion"}],"dockerImageVersionId":30775,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
